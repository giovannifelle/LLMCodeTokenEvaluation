Buongiorno prof., volevo aggiornarla riguardo al progetto e porle alcune domande in modo da essere allineati sullo svolgimento.

Per quanto riguarda la struttura volevo proprole la seguente:

PROMPT (unico per tutti i modelli/linguaggi/algoritmi): Write ONLY the pure source code for the {algorithm} algorithm in {language}. 
DO NOT INCLUDE ANY COMMENTS, explanations, docstrings, markdown formatting, or main function.
Absolutely NO #, //, /*, or any other comment syntax. Only pure code.


LISTA MODELLI: [
	deepseek-coder: 1.3b
    deepseek-coder: 6.7b
    qwen3: 1.7b
    qwen3: 8b
    qwen2.5-coder: 1.5b
    qwen2.5-coder: 7b
	starcoder2: 3b
	starcoder2: 7b
]

LISTA LINGUAGGI: [
	Popolari: python, java, c, c++, go
	Di nicchia: rust, R, julia, scala, crystal
]

Per essere precisi: rust, go e r sono una via di mezzo, però temevo che con modelli piccoli andare troppo nella nicchia si rischiasse di non ottenere risultati (comunque julia e crystal sono top 30)


LISTA ALGORITMI: ["bubble_sort", "binary_search", "fibonacci", "matrix_multiplication", "is_palindrome"]

Le domande che volevo porle sono:

- In qualche prova effettuata ho notato (non sempre) che i modelli, soprattutto i più piccoli, ignorino le istruzioni date di "NO COMMENTS" o aggiungano delimitatori di markdown anche se non richiesti. Ho pensato di procedere, ignorando questo tipo di "errore" in fase di valutazione dello script generato, ma tenerne conto nella tabella ponendo un asteristico sulle generazioni errate. In questo modo evidenzierei gli errori di inferenza, ma senza compromettere lo studio. Che ne pensa?

- Per quanto riguarda la generazione, vorrei chiedere ai modelli di generare solo gli script delle funzioni dei vari algoritmi, quindi niente funzioni main o codice di esecuzione. Questo per valutare 1:1 le funzioni nei vari linguaggi. Va bene o vuole che includa anche i vari main?

Perdoni il messaggio lungo e la ringrazio per la disponibilità. Resto a disposizione per eventuali chiarimenti o modifiche.

Cordiali saluti, 

Giovanni Felle